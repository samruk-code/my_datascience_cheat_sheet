{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c969f8c4-c945-49dd-a7d0-efb759de03a1",
   "metadata": {},
   "source": [
    "# Supervised Learning with scikit-learn\n",
    "\n",
    "\n",
    "### scikit-learn workflow\n",
    "\n",
    "```\n",
    "from sklearn.module import Model\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "model.predict(X_new)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0410233-47b2-4810-bbe5-c6f2064f185e",
   "metadata": {},
   "source": [
    "predict the label of a data point by looking at the k closest data points and taking a majority vote, larger k - can cause underfiting(less complex model), smaller k - can lead to overfitting(more complex model)\n",
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c771910-d24d-4a88-814d-2b40f25cbd4b",
   "metadata": {},
   "source": [
    "stratify - target labels proportions reflect in train and test split\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c5901-76b0-413d-821c-4b9d65ff1757",
   "metadata": {},
   "source": [
    "Linear regression $$y = mx+b $$\n",
    "```\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5d448-9d83-4d60-a2d5-1fc166bdf687",
   "metadata": {},
   "source": [
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "```\n",
    "from sklearn.metrics import root_mean_squred_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "```\n",
    "\n",
    "R-squared is quantifies the variance in target variable values explained by the independent features, values range from 0  to 1, higher the better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838bf6fe-8ecf-4db6-b557-93f6f6369f8a",
   "metadata": {},
   "source": [
    "K-fold validation for model training and validation - \n",
    "cross_val_score return highest score as a best score\n",
    "```\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb79d6-1664-4fc3-928b-3b2f1d83d185",
   "metadata": {},
   "source": [
    "L2 Regularised Regression\n",
    "-alpha - lambda value determined the size of the regularisation effect\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Ridge\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4080679-ec28-4aa3-9d25-381ae5e62765",
   "metadata": {},
   "source": [
    "L1 Regularised Regression -alpha - lambda value determined the size of the regularisation effect\n",
    "```\n",
    "from sklearn.linear_model import Lasso\n",
    "```\n",
    "Lasso can select important features of a dataset, shrinks the coefficients of less important features to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67160ea1-e2c6-41e4-9266-2106fe2c1100",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classfication_report\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02369c28-fe93-4e63-b233-65f267cb6619",
   "metadata": {},
   "source": [
    "logistic regression uses a sigmoid function get probability of geing a positive class given net input.\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8a37b-da55-4bf1-8ace-11f77904f489",
   "metadata": {},
   "source": [
    "roc_curve and roc_auc_score are help us what heppen to true possitive rate and false positive rate when varying the threshold.\n",
    "```\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f884525-a3a3-42ca-a174-404c6e4783cc",
   "metadata": {},
   "source": [
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b98d8-9ecd-4129-8e49-bbc2e64eebae",
   "metadata": {},
   "source": [
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "high precision = lower false positive rate,\n",
    "not many actual positives predicted negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1ce52-be04-4205-bde6-c76661c2beaa",
   "metadata": {},
   "source": [
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "high recall = lower false negative rate, predicted most true positive cases correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac901c-b94e-4d90-9be0-219eb886a75e",
   "metadata": {},
   "source": [
    "$$\n",
    "F1-Score = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eae6eb-5f8f-4a27-8214-504dd02aea3a",
   "metadata": {},
   "source": [
    "when doing hyper parameter tuning good practice is use training data to cross validation or keep different validation dataset avoid models overfitting to the test dataset.\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "```\n",
    "```\n",
    "best_params_\n",
    "best_score_\n",
    "```\n",
    "attributes helps get the information, GridSearch is computationally expensive compared to RandomizedSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb42c3b-cd80-41de-b6dd-8239b42dc661",
   "metadata": {},
   "source": [
    "For create dummy variables for categorical features\n",
    "```\n",
    "pandas.get_dummies()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77af0c-a550-4c16-891c-826b4968eb84",
   "metadata": {},
   "source": [
    "when doing imputation, we must split our data as training and testing set to avoid data leakage, common imputation are mean sometimes median depending on the usecase, categorical features use mode.\n",
    "\n",
    "```\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    ".fit_transform\n",
    ".transofrm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088e3f7-8d62-4a5f-b741-2d40dacec18d",
   "metadata": {},
   "source": [
    "to make all the steps as a single object\n",
    "```\n",
    "from sklearn.pipeline import Pipeline\n",
    "```\n",
    "there is a method that help hyperparamter tuning when model building is part of a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d2d67-4ba0-4840-b594-37bf02e8ab0c",
   "metadata": {},
   "source": [
    "normalizing and standardizing helps model to learn faster and accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785fc177-d580-487c-98ed-80864a7c224c",
   "metadata": {},
   "source": [
    "standardization - Subtract the mean and divide by variance All features are centered around zero and have a variance of one\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec20e54-a364-4bfc-9d4f-2dcb36eaf4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
